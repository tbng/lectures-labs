{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in Multilayer Neural Networks\n",
    "\n",
    "### Goals: \n",
    "- Diving deep: implement a real gradient descent in `Numpy`\n",
    "\n",
    "### Dataset:\n",
    "- Similar as first Lab - Digits: 10 class handwritten digits\n",
    "- [sklearn.datasets.load_digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# print(scaler.mean_)\n",
    "# print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Implementation\n",
    "\n",
    "## a) Logistic Regression\n",
    "\n",
    "In this section we will implement a logistic regression model trainable with SGD using numpy. Here are the objectives:\n",
    "\n",
    "- Implement a simple forward model with no hidden layer (equivalent to a logistic regression):\n",
    "note: shape, transpose of W with regards to course\n",
    "$y = softmax(\\mathbf{W} \\dot x + b)$\n",
    "\n",
    "- Build a predict function which returns the most probable class given an input $x$\n",
    "\n",
    "- Build an accuracy function for a batch of inputs $X$ and the corresponding expected outputs $y_{true}$\n",
    "\n",
    "- Build a grad function which computes $\\frac{d}{dW} -\\log(softmax(W \\dot x + b))$ for an $x$ and its corresponding expected output $y_{true}$ ; check that the gradients are well defined\n",
    "\n",
    "- Build a train function which uses the grad function output to update $\\mathbf{W}$ and $b$\n",
    "\n",
    "\n",
    "### One-hot encoding for class label data\n",
    "\n",
    "First let's define a helper function to compute the one hot encoding of an integer array for a fixed number of classes (similar to keras' `to_categorical`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(n_classes=10, y=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(n_classes=10, y=[0, 4, 9, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax function\n",
    "\n",
    "Now let's implement the softmax vector function:\n",
    "\n",
    "$$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # TODO:\n",
    "    expo_X = np.exp(X)\n",
    "    return expo_X / np.sum(expo_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that this works one vector at a time (and check that the components sum to one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n"
     ]
    }
   ],
   "source": [
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a naive implementation of softmax might not be able process a batch of activations in a single call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.92957576e-01 3.33100158e-04 2.24441121e-06]\n",
      " [1.65840803e-05 6.69049552e-03 9.29172262e-14]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way to implement softmax that works both for an individual vector of activations and for a batch of activation vectors at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax of a single vector:\n",
      "[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n"
     ]
    }
   ],
   "source": [
    "def softmax(X):\n",
    "    exp = np.exp(X)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "print(\"softmax of a single vector:\")\n",
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities should sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(softmax([10, 2, -3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sotfmax of 2 vectors:\n",
      "[[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n",
      " [2.47262316e-03 9.97527377e-01 1.38536042e-11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"sotfmax of 2 vectors:\")\n",
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of probabilities for each input vector of logits should some to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(softmax(X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that given the true one-hot encoded class `Y_true` and and some predicted probabilities `Y_pred` returns the negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016459296765412995\n"
     ]
    }
   ],
   "source": [
    "def nll(Y_true, Y_pred, eps=1):\n",
    "    Y_true = np.asarray(Y_true) + eps\n",
    "    Y_pred = np.asarray(Y_pred) + eps\n",
    "    \n",
    "    # TODO\n",
    "    return -np.mean(np.log(Y_true / Y_pred))\n",
    "\n",
    "\n",
    "# Make sure that it works for a simple sample at a time\n",
    "print(nll([1, 0, 0], [.99, 0.01, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the nll of a very confident yet bad prediction is a much higher positive number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0032834419509449133\n"
     ]
    }
   ],
   "source": [
    "print(nll([1, 0, 0], [0.01, 0.01, .98]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the average negative log likelihood of a group of predictions: `Y_pred` and `Y_true` can therefore be past as 2D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(Y_true, Y_pred):\n",
    "    Y_true = np.atleast_2d(Y_true)\n",
    "    Y_pred = np.atleast_2d(Y_pred)\n",
    "\n",
    "    # TODO\n",
    "    return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005486432255137665\n"
     ]
    }
   ],
   "source": [
    "# Check that the average NLL of the following 3 almost perfect\n",
    "# predictions is close to 0\n",
    "Y_true = np.array([[0, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Y_pred = np.array([[0,   1,    0],\n",
    "                   [.99, 0.01, 0],\n",
    "                   [0,   0,    1]])\n",
    "\n",
    "print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.9, 0.1]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.atleast_2d([0, 0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01005032575249135\n",
      "4.605169185988592\n",
      "0.0033501019174971905\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/numpy_nll.py\n",
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "def nll(Y_true, Y_pred):\n",
    "    Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred)\n",
    "    loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n",
    "    return -np.mean(loglikelihoods)\n",
    "\n",
    "\n",
    "# Make sure that it works for a simple sample at a time\n",
    "print(nll([1, 0, 0], [.99, 0.01, 0]))\n",
    "\n",
    "# Check that the nll of a very confident yet bad prediction\n",
    "# is very high:\n",
    "print(nll([1, 0, 0], [0.01, 0.01, .98]))\n",
    "\n",
    "# Check that the average NLL of the following 3 almost perfect\n",
    "# predictions is close to 0\n",
    "Y_true = np.array([[0, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Y_pred = np.array([[0,   1,    0],\n",
    "                   [.99, 0.01, 0],\n",
    "                   [0,   0,    1]])\n",
    "print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now study the following linear model trainable by SGD, **one sample at a time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `one_hot` not found.\n"
     ]
    }
   ],
   "source": [
    "?one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.uniform(size=(input_size, output_size),\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.b = np.random.uniform(size=output_size,\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        return softmax(Z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        y_pred = self.forward(x)\n",
    "        dnll_output =  y_pred - one_hot(self.output_size, y_true)\n",
    "        grad_W = np.outer(x, dnll_output)\n",
    "        grad_b = dnll_output\n",
    "        grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD update without momentum\n",
    "        grads = self.grad_loss(x, y)\n",
    "        self.W = self.W - learning_rate * grads[\"W\"]\n",
    "        self.b = self.b - learning_rate * grads[\"b\"]      \n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        return nll(one_hot(self.output_size, y), self.forward(x))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "??np.outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of the untrained model:\n",
      "train loss: 2.3365, train acc: 0.111, test acc: 0.130\n"
     ]
    }
   ],
   "source": [
    "# Build a model and test its forward inference\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = len(np.unique(y_train))\n",
    "lr = LogisticRegression(n_features, n_classes)\n",
    "\n",
    "print(\"Evaluation of the untrained model:\")\n",
    "train_loss = lr.loss(X_train, y_train)\n",
    "train_acc = lr.accuracy(X_train, y_train)\n",
    "test_acc = lr.accuracy(X_test, y_test)\n",
    "\n",
    "print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the randomly initialized model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_prediction(model, sample_idx=0, classes=range(10)):\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "    ax0.imshow(scaler.inverse_transform(X_test[sample_idx]).reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "               interpolation='nearest')\n",
    "    ax0.set_title(\"True image label: %d\" % y_test[sample_idx]);\n",
    "\n",
    "    ax1.bar(classes, one_hot(len(classes), y_test[sample_idx]), label='true')\n",
    "    ax1.bar(classes, model.forward(X_test[sample_idx]), label='prediction', color=\"red\")\n",
    "    ax1.set_xticks(classes)\n",
    "    prediction = model.predict(X_test[sample_idx])\n",
    "    ax1.set_title('Output probabilities (prediction: %d)'\n",
    "                  % prediction)\n",
    "    ax1.set_xlabel('Digit class')\n",
    "    ax1.legend()\n",
    "    \n",
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update #0, train loss: 2.3018, train acc: 0.119, test acc: 0.130\n",
      "Update #100, train loss: 1.2794, train acc: 0.690, test acc: 0.711\n",
      "Update #200, train loss: 0.8487, train acc: 0.858, test acc: 0.893\n",
      "Update #300, train loss: 0.6290, train acc: 0.904, test acc: 0.911\n",
      "Update #400, train loss: 0.5279, train acc: 0.910, test acc: 0.930\n",
      "Update #500, train loss: 0.4598, train acc: 0.919, test acc: 0.926\n",
      "Update #600, train loss: 0.4013, train acc: 0.929, test acc: 0.937\n",
      "Update #700, train loss: 0.3649, train acc: 0.930, test acc: 0.941\n",
      "Update #800, train loss: 0.3435, train acc: 0.938, test acc: 0.948\n",
      "Update #900, train loss: 0.3186, train acc: 0.937, test acc: 0.948\n",
      "Update #1000, train loss: 0.3006, train acc: 0.941, test acc: 0.948\n",
      "Update #1100, train loss: 0.2819, train acc: 0.944, test acc: 0.963\n",
      "Update #1200, train loss: 0.2711, train acc: 0.950, test acc: 0.959\n",
      "Update #1300, train loss: 0.2591, train acc: 0.950, test acc: 0.956\n",
      "Update #1400, train loss: 0.2477, train acc: 0.952, test acc: 0.952\n",
      "Update #1500, train loss: 0.2342, train acc: 0.951, test acc: 0.956\n"
     ]
    }
   ],
   "source": [
    "# Training for one epoch\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "    lr.train(x, y, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        train_loss = lr.loss(X_train, y_train)\n",
    "        train_acc = lr.accuracy(X_train, y_train)\n",
    "        test_acc = lr.accuracy(X_test, y_test)\n",
    "        print(\"Update #%d, train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "              % (i, train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Feedforward Multilayer\n",
    "\n",
    "The objective of this section is to implement the backpropagation algorithm (SGD with the chain rule) on a single layer neural network using the sigmoid activation function.\n",
    "\n",
    "- Implement the `sigmoid` and its element-wise derivative `dsigmoid` functions:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xlc1NX+x/HXYRv2HRUF3HdRU0SzRW0xd+u2aVmZ2Xrtdiv7Zevt1u223W7ZptfMzDK9pTfTsizL0kpTUHFD3ABFVDbZGWDg/P74kqGhjMrwHYbP8/GYB8zMmZnPjPD2cL7ne47SWiOEEMK1uJldgBBCiIYn4S6EEC5Iwl0IIVyQhLsQQrggCXchhHBBEu5CCOGCJNyFEMIFSbgLIYQLknAXQggX5GHWC4eHh+t27dqZ9fJCCNEkJSYm5mitI+prZ1q4t2vXjoSEBLNeXgghmiSlVLo97WRYRgghXJCEuxBCuCAJdyGEcEH1jrkrpeYBY4AsrXWvOu5XwExgFFAKTNZabz6XYiorK8nIyMBqtZ7LwwXg7e1NVFQUnp6eZpcihDCRPQdU5wNvAQtOc/9IoHPNZSAwq+brWcvIyCAgIIB27dph/J8hzobWmtzcXDIyMmjfvr3Z5QghTFTvsIzWei2Qd4Ym44EF2rABCFZKRZ5LMVarlbCwMAn2c6SUIiwsTP7yEUI0yJh7G+BQresZNbedEwn28yOfnxACGmaee11pUufefUqpu4C7AGJiYhrgpYUQwjlorSmrrKLIaqPIaqO43EZJ+e9fSyqqKC23UVpRxWXdWtAnOtih9TREuGcA0bWuRwGZdTXUWs8B5gDExcU1mc1bp06dykMPPUSPHj0c9hqjRo3i448/Jjj45H/wZ555Bn9/f6ZPn+6w1xZCnKyqWpNbUk5OUQV5JRXklpSTV1LB8ZIKjpdWkl9WSX5pBQVllRSUVVJktVFYVomt2r5YiwiwNIlwXw5MU0otxjiQWqC1PtIAz+s05s6d6/DXWLlypcNfQwgBJeU2DueXkZlfxpECK0cKrBwrsHKsyMqxwnKyi6zklVRQV04rBUE+ngT7eBLs60WIrxftwvwI8vEk0MeDAG9PArw98Ld4EODtgZ+XB34W47qfxQM/izveHu64uTl++NSeqZCLgKFAuFIqA/gb4AmgtZ4NrMSYBrkPYyrk7Y4qtjGUlJRwww03kJGRQVVVFU899RSzZs3iX//6F3Fxcbz33nu89NJLtG7dms6dO2OxWHjrrbeYPHkyPj4+7N69m/T0dN5//30++OAD1q9fz8CBA5k/fz4AixYt4p///Cdaa0aPHs1LL70E/L4cQ3h4OM8//zwLFiwgOjqaiIgI+vfvb+InIkTTorUmr6SC1JwSDuSUkJ5bQnpuKQfzSjmUV8rx0sqT2isF4f4WWgV60ybYm77RQUT4W4gIsBDmbyHMz4swfwuhfl4E+Xji3gjB3BDqDXet9cR67tfAnxusohp/X7GTXZmFDfqcPVoH8rexPc/Y5uuvv6Z169Z8+eWXABQUFDBr1iwAMjMzee6559i8eTMBAQFcdtll9OnT58Rjjx8/zvfff8/y5csZO3YsP//8M3PnzmXAgAFs3bqVFi1a8Oijj5KYmEhISAjDhw9n2bJlXH311SeeIzExkcWLF7NlyxZsNhv9+vWTcBfiNPJKKth9pJDko0Xsyypi77Fi9mYVU1D2e4B7uCnahPgQE+pLr9hIokJ8aBNsXCKDfWgRYMHT3fXO5zRt4TBnFRsby/Tp03n00UcZM2YMl1xyyYn7Nm7cyJAhQwgNDQXg+uuvZ8+ePSfuHzt2LEopYmNjadmyJbGxsQD07NmTtLQ00tPTGTp0KBERxoJuN998M2vXrj0p3NetW8c111yDr68vAOPGjXP4exaiKcgpLifpUD7bMgrYcbiAHZkFHCssP3F/qJ8XnVv4M6Z3JB0j/Gkf4Uf7MD+iQnzwcMHwro/Thnt9PWxH6dKlC4mJiaxcuZLHHnuM4cOHn7jP+CPl9CwWCwBubm4nvv/tus1mw8PDvo9bpjOK5q66WrM3q5iNaXkkpOWx5WA+B/NKAWMYpVOEP4M7htMjMpDukYF0bRVARIClnmdtXpw23M2SmZlJaGgokyZNwt/f/8RYOUB8fDwPPvggx48fJyAggKVLl57ondtj4MCBPPDAA+Tk5BASEsKiRYu4//77T2pz6aWXMnnyZGbMmIHNZmPFihXcfffdDfX2hHBKWmtSc0r4eX8uP+/NYUNqLvk1Y+MtAy30iwlh0qAY+kaH0LN1IH4Wia76yCd0iu3bt/PII4/g5uaGp6cns2bNOjENsU2bNjz++OMMHDiQ1q1b06NHD4KCgux+7sjISF544QWGDRuG1ppRo0Yxfvz4k9r069ePG2+8kb59+9K2bduThoWEcCXWyirW789lTUoWa1KyOJRXBkCbYB+u7N6SgR3CiG8XSnSoj/w1ew5UfUMNjhIXF6dP3awjOTmZ7t27m1KPvYqLi/H398dms3HNNdcwZcoUrrnmGrPLOklT+BxF81RoreT75CxW7TzKDynZlFVW4ePpzkWdwhnSNYJLOoXTNsxXwvwMlFKJWuu4+tpJz/0sPfPMM6xevRqr1crw4cNPOhgqhPgja2UV3yVnsTzpMGtSsqmwVdMiwMK1/dswvEcr4tuH4u3pbnaZLkfC/Sz961//MrsEIZye1prNB/NZkniIL5KOUFRuIyLAwk3xMYztE8kF0SGNciJPcybhLoRoMEXWSv63+TAfbUhnb1YxPp7ujIxtxbX9ohjUIazJnADkCiTchRDnLTWnhHk/pbJ0cwalFVX0iQrixT/FMqZPa/xlZosp5FMXQpyzxPQ8/vPjAb5NPoanmxvj+rbmlkFtHb4olqifhLsQ4qxordlwII83vtvL+gO5BPt6cv+wTtxyYTs5kciJSLjX40xL7i5fvpxdu3YxY8YMh73+7Nmz8fX15dZbbz3p9rS0NMaMGcOOHTsc9tpCnCohLY+XV6WwMTWPiAALT43pwcT4aHy9JEqcjfyLnIdx48Y5fO2Xe+65x6HPL4Q99hwr4uWvd7M6OYuIAAvPjO3BhPgYmcLoxJrfajp2eP755+natStXXHEFKSkpALzxxhv06NGD3r17M2HCBADmz5/PtGnTANi/fz+DBg1iwIABPP300/j7+wPwww8/MGTIEG644Qa6dOnCjBkzWLhwIfHx8cTGxrJ//34A0tPTufzyy+nduzeXX345Bw8eBIy/HH6bfpmYmEifPn248MILefvttxv1MxHNU15JBU8u286I19fy64E8HrmqKz8+MpTJF7WXYHdyzttz/2oGHN3esM/ZKhZGvnjGJqdbcvfFF18kNTUVi8VCfn7+Hx73wAMP8MADDzBx4kRmz5590n1JSUkkJycTGhpKhw4dmDp1Khs3bmTmzJm8+eabvP7660ybNo1bb72V2267jXnz5vGXv/yFZcuWnfQ8t99+O2+++SZDhgzhkUceOf/PQ4jTqKrWfLQhnX9/u4fichu3XtiOBy7vTIifl9mlCTtJz/0UtZfcDQwMPDHs0rt3b26++WY++uijOld3XL9+Pddffz0AN91000n3DRgwgMjISCwWCx07djyx0mRsbCxpaWknHv/b42655RZ++umnk56joKCA/Px8hgwZcqKNEI6w43ABV7/9M39bvpPYNkF89cAlPDOupwR7E+O8Pfd6etiOVNe6Fl9++SVr165l+fLlPPfcc+zcudPu5zt1+d/aSwPbbDa7atBay3obwqHKKqp49ZsU5v2cSpi/hbdv6seo2Fbyc9dESc/9FJdeeimfffYZZWVlFBUVsWLFCqqrqzl06BDDhg3j5ZdfJj8/n+Li4pMeN2jQIJYuXQrA4sWLz/p1Bw8efOJxCxcu5OKLLz7p/uDgYIKCgk706BcuXHgub0+IOiWm5zHqjXXM/SmVCfExrH5oCKN7R0qwN2HO23M3SV1L7iqlmDRpEgUFBWitefDBBwkOPvkkjddff51Jkybx6quvMnr06LNaChiMA7ZTpkzhlVdeISIigvfff/8Pbd5//32mTJmCr68vV1111Xm9TyEAym1VvPbtXuas3U9kkA+L7hzEhR3DzC5LNABZ8reBlJaW4uNjrDu9ePFiFi1axOeff25KLU35cxSNJzWnhPsXbWbH4UImDIjmyTE9ZKmAJkCW/G1kiYmJTJs2Da01wcHBzJs3z+yShDitz7Zk8ORnO/Bwd2POLf0Z3rOV2SWJBibh3kAuueQSkpKSzC5DiDMqt1XxzPKdLNp4iAHtQpg54QJaB/uYXZZwAKcLd5kVcn7MGmYTzu9IQRn3fLSZpEP53Du0Iw9f2QUPd5lT4aqcKty9vb3Jzc0lLCxMAv4caK3Jzc3F29vb7FKEk9mUlse9HyVSVlHF7En9GNEr0uyShIM5VbhHRUWRkZFBdna22aU0Wd7e3kRFRZldhnAi/9ucwYyl22kTYsyG6dwywOySRCNwqnD39PSkffv2ZpchhEuorta8tnoPb36/jws7hDFrUj+CfeUs0+bCqcJdCNEwKmzV/N+SJJZtzeTGuGieu7oXXh4yvt6cSLgL4WJKym3cu3Aza/dk88hVXblvaEc5htUMSbgL4ULySiq4ff4mtmfk89K1sdw4IMbskoRJJNyFcBFZRVZufvdXDuaV8p9b4riyR0uzSxImknAXwgUcLbBy07sbOFpoZf7t8bI+jLBvVUil1AilVIpSap9S6g8bhiqlYpRSa5RSW5RS25RSoxq+VCFEXQ7nl3HjnPVkFZWzYIoEuzDUG+5KKXfgbWAk0AOYqJTqcUqzJ4FPtNYXABOAdxq6UCHEHx0pKGPCnPXklVTw4R3xxLULNbsk4STs6bnHA/u01ge01hXAYmD8KW00EFjzfRCQ2XAlCiHq8tsY+/GSSj66YyAXxISYXZJwIvaMubcBDtW6ngEMPKXNM8A3Sqn7AT/gigapTghRp7ySCibN/ZWjhVYWTImnT3Rw/Q8SzYo9Pfe6JsieujrVRGC+1joKGAV8qJT6w3Mrpe5SSiUopRJkiQEhzk2RtZJb5/1Kem4pc2+Lk6EYUSd7wj0DiK51PYo/DrvcAXwCoLVeD3gD4ac+kdZ6jtY6TmsdFxERcW4VC9GMlduquPvDRJKPFDF7Un8Gd/zDr5kQgH3hvgnorJRqr5TywjhguvyUNgeBywGUUt0xwl265kI0oKpqzUP/TeKX/bm8cl1vhnVrYXZJwonVG+5aaxswDVgFJGPMitmplHpWKTWuptnDwJ1KqSRgETBZy8LiQjQYrTV/X7GTL7cf4YlR3flTP1n5U5yZXScxaa1XAitPue3pWt/vAi5q2NKEEL9576dUFqxP585L2nPnpR3MLkc0AbJMnBBO7usdR3l+ZTIje7XisZGy8bmwj4S7EE4s6VA+f/3vFvpEBfPajX1xc5PVHYV9JNyFcFJHCsqYuiCBcH8Lc2+Lw9vT3eySRBMi4S6EE7JWGlMeS8ttzJs8gHB/i9kliSZGVoUUwslorZmxdBvbMgp499Y4usiep+IcSM9dCCczZ+0Blm3N5OEru8ia7OKcSbgL4UR+3pfDS1/vZnRsJNMu62R2OaIJk3AXwklk5pdx/6ItdIzw5+Xresu+p+K8SLgL4QQqbNXct3AzFbZqZt/SHz+LHA4T50d+goRwAv/4chdbD+Uz6+Z+dIzwN7sc4QKk5y6Eyb7YlnliaYGRsZFmlyNchIS7ECZKzy1hxtLt9IsJ5v9GdDO7HOFCJNyFMEm5rYppH2/B3U3xxsQL8HSXX0fRcGTMXQiTvPjVbrYfLmDOLf2JCvE1uxzhYqSrIIQJvks+xvs/pzF5cDuG92xldjnCBUm4C9HIsoqsPLJkG90jA3lslIyzC8eQcBeiEVVXa6Z/uo2SchtvTOiLxUNWehSOIeEuRCP6YH0aa/dk8+To7nSWBcGEA0m4C9FIUo4W8cJXu7m8WwsmDWprdjnCxUm4C9EIKmzVPPjfrQR6e/CSrBsjGoFMhRSiEbzx3V52HSlkzi39ZeMN0Sik5y6Eg20+eJx3ftjH9f2jZNqjaDQS7kI4UGmFjYc/SSIyyIenx/YwuxzRjMiwjBAO9MqqFFJzSvj4zoEEeHuaXY5oRqTnLoSDbEzNY/4vadx2YVsGdww3uxzRzEi4C+EAZRVVPLIkiegQXx4dKWehisYnwzJCOMArq1JIzy1l0Z2D8PWSXzPR+KTnLkQDS0jL4/1fUrn1wrZc2DHM7HJEMyXhLkQDslZW8X9Lt9E6yIdHZfMNYSL5e1GIBvTm93s5kF3Cginxssm1MJX03IVoIDszC/jPjwe4rn8Ul3aJMLsc0czZFe5KqRFKqRSl1D6l1IzTtLlBKbVLKbVTKfVxw5YphHOzVVXz6NJtBPt68eTo7maXI0T9wzJKKXfgbeBKIAPYpJRarrXeVatNZ+Ax4CKt9XGlVAtHFSyEM3rvp1R2HC7knZv7EezrZXY5QtjVc48H9mmtD2itK4DFwPhT2twJvK21Pg6gtc5q2DKFcF4Hc0t5bfUeruzRkpG9ZO0Y4RzsCfc2wKFa1zNqbqutC9BFKfWzUmqDUmpEQxUohDPTWvPEsu14uLnx7PiespSvcBr2HM6v66dV1/E8nYGhQBSwTinVS2udf9ITKXUXcBdATEzMWRcrhLP5fGsm6/bm8Oz4nkQG+ZhdjhAn2NNzzwCia12PAjLraPO51rpSa50KpGCE/Um01nO01nFa67iICJlNIJq24yUVPPvFLi6ICebmgbKzknAu9oT7JqCzUqq9UsoLmAAsP6XNMmAYgFIqHGOY5kBDFiqEs/nnymQKyyp54U+xuLvJcIxwLvWGu9baBkwDVgHJwCda651KqWeVUuNqmq0CcpVSu4A1wCNa61xHFS2E2TYcyOXTxAzuvLQD3VoFml2OEH+gtD51+LxxxMXF6YSEBFNeW4jzUW6rYtTMdVRUVfPNX4fg4+VudkmiGVFKJWqt4+prJ+dHC3GW5vx4gP3ZJbx/+wAJduG0ZPkBIc5CWk4Jb67Zx+jYSIZ1lXP1hPOScBfCTlprnvp8BxZ3N9kPVTg9CXch7PTFtiOs25vD9Ku60jLQ2+xyhDgjCXch7FBoreS5L3YR2yaISYNkTrtwfnJAVQg7/PubPWQXlzP3tjiZ0y6aBOm5C1GPHYcLWLA+jUkD29I7KtjscoSwi4S7EGdQVa154rPthPpZmH5VV7PLEcJuEu5CnMHiTQdJyijgydHdCfLxNLscIewm4S7EaeQUl/Py1ykM6hDK+L6tzS5HiLMi4S7Eabz41W5Kym384+pesk67aHIk3IWow8bUPJbULAzWqUWA2eUIcdYk3IU4RWVVNU8t20GbYB/uv6yT2eUIcU4k3IU4xfyf00g5VsTfxvbA10tOBRFNk4S7ELUcKSjj9dV7uKxbC67s0dLscoQ4ZxLuQtTyjy+SsVVrnhkrm12Lpk3CXYgaa/dk8+X2I0wb1omYMF+zyxHivEi4CwFYK6v42/KdtA/3464hHcwuR4jzJkeLhADmrD1Aak4JC6bEY/GQ3ZVE0yc9d9HspeeW8NaafYzuHcmlXSLMLkeIBiHhLpo1rTXPLN+Jp5viqdGyu5JwHRLuollbtfMYa1KyefDKLrQKkt2VhOuQcBfNVkm5jWdX7KRbqwAmD25ndjlCNCgJd9FszfxuL5kFVv5xdS883OVXQbgW+YkWzdLuo4W891MqN8ZFE9cu1OxyhGhwEu6i2amu1jz52Q4CvT2YMbKb2eUI4RAS7qLZWZKYQUL6cR4b1Z0QPy+zyxHCISTcRbOSV1LBC18lE98ulOv6RZldjhAOI+EumpV/rkymyGrjH9f0ws1NFgYTrkvCXTQb6/fnnthdqUtL2V1JuDYJd9EslNuqeGLZdqJDffjLZZ3NLkcIh7Mr3JVSI5RSKUqpfUqpGWdod51SSiul4hquRCHO339+PMCB7BKeHd8LHy9ZGEy4vnrDXSnlDrwNjAR6ABOVUn9YhEMpFQD8Bfi1oYsU4nwcyC42FgaLjWRY1xZmlyNEo7Cn5x4P7NNaH9BaVwCLgfF1tHsOeBmwNmB9QpwXrTWPf7Ydi4cbfxsrC4OJ5sOecG8DHKp1PaPmthOUUhcA0VrrLxqwNiHO26eJGWw4kMdjI7vTIlAWBhPNhz3hXtd8MX3iTqXcgNeAh+t9IqXuUkolKKUSsrOz7a9SiHOQU1zO818mM6BdCBMGRJtdjhCNyp5wzwBq/2ZEAZm1rgcAvYAflFJpwCBgeV0HVbXWc7TWcVrruIgI2RRBONZzX+yitMLGC3+KlTntotmxJ9w3AZ2VUu2VUl7ABGD5b3dqrQu01uFa63Za63bABmCc1jrBIRULYYc1KVl8vjWTe4d2olMLmdMump96w11rbQOmAauAZOATrfVOpdSzSqlxji5QiLNVXG7jif9tp1MLf/48rKPZ5QhhCrs2yNZarwRWnnLb06dpO/T8yxLi3L389W6OFFpZcs9g2exaNFtyhqpwKZvS8liwPp3Jg9vRv22I2eUIYRoJd+EyrJVVPLp0G1EhPkwf3tXscoQwlV3DMkI0Ba+v3suB7BI+vCMeP4v8aIvmTXruwiVsOXicOWv3MzE+mks6yzRbISTcRZNnrazikSXbaBXozeOjuptdjhBOQf52FU3ezO/2si+rmA+mxBPg7Wl2OUI4Bem5iyZt66F8/vPjfm6Mi2ZIFxmOEeI3Eu6iySqrqOKh/26lVaA3T4yR4RghapNhGdFkvfT1bg7klPDx1IEEynCMECeRnrtokn7am8P8X9K4/aJ2DO4UbnY5QjgdCXfR5BSUVfLIkiQ6RPjx6IhuZpcjhFOSYRnRpGiteWrZDrKKyll672C8PWXtGCHqIj130aQs23qY5UmZ/PXyzvSNDja7HCGcloS7aDIO5pby1LKdDGgXwn3DOpldjhBOTcJdNAm2qmr++t8tKOC1G/viLjsrCXFGMuYumoQ3vtvL5oP5zJzQl6gQX7PLEcLpSc9dOL2f9+Xw5pp9XNsvivF925hdjhBNgoS7cGrZReU8sHgrHcL9eO7qnmaXI0STIcMywmlVV2se+mQrRdZKPpoaj6+X/LgKYS/5bRFO650f9rFubw4v/CmWbq0CzS5HiCZFhmWEU1q3N5tXv93D+L6tmTAg2uxyhGhyJNyF0zmcX8ZfFm2hcwt/XvhTLErJtEchzpaEu3Aq5bYq7vsokcoqzexJ/WWcXYhzJL85wmlorXlm+U6SMgqYPakfHSL8zS5JiCZLwl04jY82pLNo4yHuHdqREb0iHftiWkNJNmTvhsIjUJoDJTlgK/+9jac3+IaDXzgEtoaIbsb3QjQBEu7CKazfn8vfV+zism4tmD68a8O/QHkxZGyE9F8gfT0c2wHW/JPbuHmAh8/v1ytLQVed3MY3DFr2hJjB0HYwRA0ALzljVjgfCXdhukN5pdy3MJG2Yb68PqEB140pzoLdXxqX1B+hqgKUO0T2hp7XGD3xiC4QFAN+YeAdDLUP3lZXG/8BlOZC/kHITjF6+pmb4ceXAA0e3tBhGHQbDV1HGc8jhBOQcBemKrRWcscHm7BVa969Ne78t8uzlUPKV7DlI9j/HehqCGkH8XdBx8sgOh4sAfY9l5sb+IYal/DO0Ony3++zFsChjbBvtfGfx56vjJ5/56vggknQ+Upwl63/hHmU1tqUF46Li9MJCQmmvLZwDpVV1UyZv4n1+3P5YEo8F53PdnmFR2DTu5A43+hpB7SGvhOh17XQosfJPfKGpjUc3Qbbl0DSYijJAr8WMOAOiLsD/CMc99qi2VFKJWqt4+ptJ+EuzKC15vHPtrNo4yFevrY3N5zriUpZu+Gn12DHUqi2GUMjcVOg4zBwM2GXpqpKoze/6T3Y9y24W6D3DXDxgxDWsfHrES7H3nCXYRlhitk/HmDRxkP8eVjHcwv2Yzth7Suwcxl4+hqBPvBu8wPU3RO6jjQu2SmwYRYkLYKtCyH2Brh0ujHEI4SDSc9dNLoliRlM/zSJsX1aM/PGvridzQHUvFT4/h+wYwl4BcDAu2DQn537QGbRMfjlDUiYBzYr9JkIQx+DYFlWQZy9Bh2WUUqNAGYC7sBcrfWLp9z/EDAVsAHZwBStdfqZnlPCvXn6LvkYd32YyKAOocybPACLh51DJyU58OPLRkC6ecCF98GF04yDnU1FcTb8/DpsnAMoiL/T6Mn7hJhdmWhCGizclVLuwB7gSiAD2ARM1FrvqtVmGPCr1rpUKXUvMFRrfeOZnlfCvflJTM/j5rm/0rlFAIvuGoS/xY5RQVuFEYY/vgwVxdDvVhjyKAQ6+CQnR8o/CGteMIZrfIJh2BPQ/3Zwl1FSUT97w92etWXigX1a6wNa6wpgMTC+dgOt9RqtdWnN1Q1A1NkWLFzbjsMFTH5/E60CvXn/9gH2Bfueb+CdQfDNExA9AO5bD2Nfb9rBDhAcA9fMgnvWQctesHI6zL4IDvxgdmXChdgT7m2AQ7WuZ9Tcdjp3AF/VdYdS6i6lVIJSKiE7O9v+KkWTlnK0iFve+5VAb08+mjqQcH/LmR9wPB0W3QQfX29MYbzpU5i0FCIccOaqmVrFwm0r4MaPoLIMFoyHT2+HwkyzKxMuwJ6/A+s62lXnWI5SahIQBwyp636t9RxgDhjDMnbWKJqw/dnF3Dx3A14ebiycOvDMm1vbyo0Dj2v/ZZxJesXfYdB94OHVeAU3NqWg+1jodAX8PNOY1rlnFQx7DAbeIydCiXNmT889A6h9WD8K+EPXQil1BfAEME5rXX7q/aL52ZdVzE3vbgBg4dRBtAv3O33j1HUw+2JjJkyXq2DaRrj4r64d7LV5+sDQGXDfBmh/CXzzJMwZCoc2mV2ZaKLsCfdNQGelVHullBcwAVheu4FS6gLgPxjBntXwZYqmJuVoERPmrKeqWrNw6iA6tTjN8r0lufDZvfDBGKPnfvMSuGEBBDXTwzah7WHiYmOopuw4vHclrPgrlOXX/1ghaql3WEZrbVNKTQNWYUyFnKe13qmUehZI0FovB14B/IFPa3bNOai1HufAuoUT25lZwKS5v+Lp7sbHd54m2LU2TtVY+R7pAAAO90lEQVRf9TiUF8LFD8Glj8gKi/D7UE2Hocasml9nQcpKGPkS9LjasUspCJchJzGJBrUpLY875m/C3+LBx3eeZigmdz988aCxUmNUPIydCS17NH6xTUXmFlj+F2P9ms5XwehX5QSoZqwhp0IKYZfVu44xae6vhPtb+O/dF/4x2KsqYd2rMGuwEVijX4UpqyTY69P6ArhzDQx/HtLWwdsDYf07UF1V/2NFsyXhLhrEJwmHuPujRLq2CuDTey4kOvSU4ZWMBPjPEPjuWWM53D9vhAFTjWV1Rf3cPWDwNOOAa9vBsOoxmHs5HEkyuzLhpOQ3S5wXrTX//iaF/1uyjcEdw1h05yDCas9jtxbCl9Nh7hXGAcIJHxsHC5v6iUhmCWkLN38K174HBRkwZxisegIqSsyuTDgZOd9ZnDNrZRWPLNnGiqRMboiL4h9Xx+LlUdNf0BqSl8NXj0LRUWPFxsuetH+jDHF6SkHsdcbmId/+Dda/BbuWG8NcXYabXZ1wEtJzF+ckq9DKxHc3sCIpk0dHdOOla3v/Huz5B2HRBPjkVmOD6anfGTM9JNgblk8IjHsDbv/KmCf/8fXGZ154xOzKhBOQnrs4a5vS8rhv4WZKym3MntSPEb1qhlhsFUYv8seXQbkZBwAH3iMLYjla28Fwz0/wy0zj7N5938Owx42tBeWzb7ak5y7sprVmwfo0Js7ZgJ+XO5/dd9HvwX7gR2Pxq+/+bgwX/PlX4wCghEvj8PAyzhO4bz3EDDQOuM4ZAgc3mF2ZMImEu7BLQVkl0z7ewtOf72RIlwg+n3YxXVsFGAf1Pp0MC8ZBVYWxyNeEhTIP2yyhHWrO8v3QOIA97yr4393GcQ/RrEi3StRry8Hj3L9oC0cKrDw6oht3X9oBt6pyWPs6rPs36GpjZ6GLHjDGfoW5lIIe46DjZcZ5Bb+8Cbu/hKGPQvzdzWe9nmZOzlAVp1VZVc07a/bz5vd7aRnozRsTL6B/TDDsWgbfPA0FB43T5Ic/b0zRE84pdz98PQP2fgOhHeGq56HLCFnGoImSDbLFedl7rIiHP01iW0YB4/u25tnxvQjKTYL3n4SD66FlLFy9Atpfanapoj5hHY258Xu/NdbyWTQB2g+B4c9BZB+zqxMOIuEuTlJhq+bddQeY+d1e/C0evHNzP0a1LoUVU40eu18LGPsGXDAJ3Ozc/1Q4h85XGouRJcyDH14wzhjufYNx/kFwjNnViQYmwzLihIS0PB7/bDt7jhUzKrYVzw0LIyzxddjyIbhbYPD9xsVymuV7RdNRlm9s1r1hlnHMJG4KXPIw+LcwuzJRjwbbINtRJNydR3ZROa+s2s0nCRm0DvLmxRGRXJq1EDa+a/zi97/NmGYX0MrsUkVDK8gwevFbF4GHxTgvYfD94BtqdmXiNCTcRb0qbNXM/yWVN77bR7mtimkDArnP60s8N78PVeXQe4IxwyKkndmlCkfL2Qc//BN2LAUvf+MEqAungV+Y2ZWJU0i4i9OqrtYsT8rk1W9TOJRXxnUdq3gq9HuCkhcZc9Vjb4BLp0N4Z7NLFY3t2C5Y+zLsXAaevhB3u7GPbVAbsysTNSTcxR9orVmdnMWr36Sw+2gRYyJyeCp0NS0PfmksF9D7RrjkIWN2hWjesnYbc+R3LP39Z2PwNGjR3ezKmj0Jd3FCdbXmm11HeeO7few+ks+EwJ08FLCa8NxNxp/g/SdL70zU7Xi6sV7Q5g/BVgYdhsGFf4aOl8ta/CaRcBdYK6tYtuUwc39K5XjWYe4O+IWbPb7Hr+wwBEUby/BecAv4BJtdqnB2pXnGFMqN70LxUWOZg7g7oO9NcvC1kUm4N2PHCq0s2niQhetT6Vq2hTv9fuIS23rctA3aXWLsgNRtjCzqJc6erQJ2fQ6b5sKhDeDhDT3GG52EthdJb74RyBmqzUx1tWb9gVwW/prO3p2bGev2E19ZfiHcKwvtHozqN9WYyxzR1exSRVPm4QW9rzcuR3dAwnuwfQls+68xq6rPTcZ9oR3MrrTZk557E3cwt5SlmzNYm7CVfsU/8ifP9fRkP1q5oToMNXpUXUeBp7fJlQqXVVEKySuMk93SfgI0RMUbu0V1HydbKjYwGZZxYccKrXy57QgbNycQdWwNo9x/pZ/bPgCqW/bGre8E6HWtnHQkGl9Bxu89+axdgIKYQcbQTddRssBcA5BwdzGpOSWs3n6I9KQfaJ3zM1e4JdLF7TAAFRG98Iq9BnpeI9MYhfPITjHmy+9aVhP0QMte0HUkdLoSouJkfaJzIOHexJVVVLEpNZft2xKp3LeGrqWbuchtB4GqjCrlTnnrgfjGjjN+UeQMUuHscvdDykrYvdI4EKurwTsYOg4zVqjsMARC2ssyxHaQcG9irJVVbDt0nD07Einbv46WxzczQCUTqfIAKPZuBR0vw7/nSOMXwTvI5IqFOEdlx2H/Gti32vhalGncHhRtzLhpO9i4hHWSsK+DhLuTO1ZoZeee/WTv3QCHtxBZuI0+ai9BqhSAYs8wSlvFE9zzCrw6DzNmH8gPunA1WkPOXkj90bikr4fSHOM+n1CIGmBc2lwArfvJnHpkKqTTqK7WZOQWk75/J8fTkuDIdoIKd9OxOpXLlPFDXI0ix689Ra3G4NH1Ivw6X4J/aAf8JcyFq1MKIroYl/g7fw/7g+shYyMc2gR7V/3ePjgGWvWGVrHGpUUPCG4r8+vrIOHeQCqrqsk4lktW+i4KD++mKisF74IDRFjT6EgGMaoSgCrcyPaKpiQkjoyY/kR0HYQlqi8tvANNfgdCOIHaYd//NuM2awFkboXMLcbl2A5jT1hqRh08fY3zN8K7GovdhXcxhnRC2zfrPX1lWMZO1dWa3IIicjJTKThyAGtuGtV56XgVZxBQdpiWVUdopY6f9Jhs9xYU+LajMqwbvlG9aNGxLz5tYsHL16R3IYSLqCgxVrDMToasZGM2Ts5eKDx8crvANsaEg5B2Rg8/OAaCooxLYJsmuVm4DMvYqdJWRX5eNgU5Ryk5foSy40ewFR5DFx3DvTQLizWbgIpswqpziVCFRNR6bDWKXLdw8i2RZPsPJie0I76RXQiP6UZgVA8ivPxOai+EaCBefhA9wLjUVl5khHzeAchLhbz9cDzt5AO3tflFQGBrCIgE/5bGuSH+LYztJP1bGPf7hhkTGJrYMKld4a6UGgHMBNyBuVrrF0+53wIsAPoDucCNWuu0hi21brq6mtLSEkqLCygrLsBakk95cT4VJQXYSo9TVVaILitAWfNxL8/Ho6IQb1s+vrZCAnQhQbqYCFX1hxCu0op8t2AK3EMp821Jul9f0gMi8QqJwr9lB8LadCSgRVsiPCwS4EI4C0sAtOlnXE5VaTV69gWHIP8QFGYa1wszoeAwHE6EkhxODPfU5uZpHMz1Ca35GmIsuOcdbHzvHQSWQPAONL5aAn6/ePkbu1w18n8O9Ya7UsodeBu4EsgANimllmutd9VqdgdwXGvdSSk1AXgJuNERBf+6dCaRO2bjra14ayu+WPFT1fjV87hSLBQrf0rd/CnzCKLArz15liCqfcJx8wvDIyAC75CW+Ie2JiiiNX4hrQhz90T2oRHCRXh6Gyf5nelEv6pKI+BLsqA4G0qyjdk7pbnG7WXHjUvufrDmG3vR2srqf203D/D0M/7i8PKFoY8ZyzM4kD0993hgn9b6AIBSajEwHqgd7uOBZ2q+XwK8pZRS2gED+pbACLL9u1Lt4Uu1p59xMMXij7IE4O7tj4dvMJ6+QVj8gvANDMUvMAy/oFB8PS3ISLcQ4ozcPY21cM5mPRxbOVgLjQO/1gKoKDKGh8qLoLy45nqxcZygssT42ghTOu0J9zbAoVrXM4CBp2ujtbYppQqAMCCndiOl1F3AXQAxMTHnVHDfK2+CK286p8cKIUSD87CAf4RxcSL2TA6ta6Do1B65PW3QWs/RWsdpreMiIpzrgxBCCFdiT7hnANG1rkcBpx52PtFGKeUBBAF5DVGgEEKIs2dPuG8COiul2iulvIAJwPJT2iwHas444Drge0eMtwshhLBPvWPuNWPo04BVGFMh52mtdyqlngUStNbLgfeAD5VS+zB67BMcWbQQQogzs2ueu9Z6JbDylNuervW9Fbi+YUsTQghxrmS1HSGEcEES7kII4YIk3IUQwgWZtiqkUiobSDflxc9POKecnNUMNLf33NzeL8h7bkraaq3rPVHItHBvqpRSCfYst+lKmtt7bm7vF+Q9uyIZlhFCCBck4S6EEC5Iwv3szTG7ABM0t/fc3N4vyHt2OTLmLoQQLkh67kII4YIk3M+DUmq6UkorpcLNrsWRlFKvKKV2K6W2KaU+U0oFm12ToyilRiilUpRS+5RSM8yux9GUUtFKqTVKqWSl1E6l1ANm19RYlFLuSqktSqkvzK7FESTcz5FSKhpj68GDZtfSCL4FemmtewN7gMdMrscham0pORLoAUxUSvUwtyqHswEPa627A4OAPzeD9/ybB4Bks4twFAn3c/ca8H/UuZuua9Faf6O1ttVc3YCxpr8rOrGlpNa6AvhtS0mXpbU+orXeXPN9EUbYtTG3KsdTSkUBo4G5ZtfiKBLu50ApNQ44rLVOMrsWE0wBvjK7CAepa0tJlw+63yil2gEXAL+aW0mjeB2jc1ZtdiGOYteSv82RUmo10KqOu54AHgeGN25FjnWm96u1/rymzRMYf8YvbMzaGpFd20W6IqWUP7AU+KvWutDsehxJKTUGyNJaJyqlhppdj6NIuJ+G1vqKum5XSsUC7YEkpRQYQxSblVLxWuujjVhigzrd+/2NUuo2YAxwuQvvsmXPlpIuRynliRHsC7XW/zO7nkZwETBOKTUK8AYClVIfaa0nmVxXg5J57udJKZUGxGmtm+ICRHZRSo0A/g0M0Vpnm12Po9Ts/7sHuBw4jLHF5E1a652mFuZAyuihfADkaa3/anY9ja2m5z5daz3G7Foamoy5C3u8BQQA3yqltiqlZptdkCPUHDT+bUvJZOATVw72GhcBtwCX1fzbbq3p0YomTnruQgjhgqTnLoQQLkjCXQghXJCEuxBCuCAJdyGEcEES7kII4YIk3IUQwgVJuAshhAuScBdCCBf0/6OrL/habWuFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(X):\n",
    "    # TODO\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    # TODO\n",
    "    return sigmoid(X) * (1 - sigmoid(X))\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sigmoid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement `forward` and `forward_keep_all` functions for a model with a hidden layer with a sigmoid activation function:\n",
    "  - $\\mathbf{h} = sigmoid(\\mathbf{W}^h \\mathbf{x} + \\mathbf{b^h})$\n",
    "  - $\\mathbf{y} = softmax(\\mathbf{W}^o \\mathbf{h} + \\mathbf{b^o})$\n",
    "\n",
    "- Notes: \n",
    "  - try to keep the code as similar as possible as the previous one;\n",
    "  - `forward_keep_activations` is similar to forward, but also returns hidden activations and pre activations;\n",
    "\n",
    "- Update the grad function to compute all gradients; check that the gradients are well defined;\n",
    "\n",
    "- Implement the `train` and `loss` functions.\n",
    "\n",
    "**Bonus**: reimplementing all from scratch only using the lecture slides but without looking at the solution of the `LogisticRegression` is an excellent exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # TODO\n",
    "        self.W_h = np.random.uniform(size=(input_size, hidden_size),high=0.1, low=-0.1)\n",
    "        self.b_h = np.random.uniform(size=hidden_size, high=0.1, low=-0.1)\n",
    "        self.W_o = np.random.uniform(size=(hidden_size, output_size), high=0.1, low=-0.1)\n",
    "        self.b_o = np.random.uniform(size=(output_size), high=0.1, low=-0.1)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO\n",
    "        output_hidden = np.dot(X, self.W_h) + self.b_h\n",
    "        output_sigmoid = sigmoid(output_hidden)\n",
    "        Z = np.dot(output_hidden, self.W_o) + self.b_o\n",
    "        return softmax(Z)\n",
    "    \n",
    "    def forward_keep_activations(self, X):\n",
    "        # TODO\n",
    "        z_h = np.dot(X, self.W_h) + self.b_h\n",
    "        h = sigmoid(z_h)\n",
    "        y = np.dot(z_h, self.W_o) + self.b_o\n",
    "        return y, h, z_h\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        y_pred = self.forward(X)\n",
    "        return nll(y, p_pred) \n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        y_pred, h, z_h = self.forward_keep_activations(x)\n",
    "        dnll_output =  y_pred - one_hot(self.output_size, y_true)\n",
    "        grad_W_o = np.outer(x, dnll_output)\n",
    "        grad_b_o = dnll_output\n",
    "        grad_W_h = np.dot(dnll_output, np.transpose(self.W_h))\n",
    "        grad_z_h = np.transpose(grad_W_h * dsigmoid(z_h))\n",
    "        grads = {\"W_h\": grad_W_H, \"b\": grad_b}\n",
    "        return grads\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/neural_net.py\n",
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W_h = np.random.uniform(\n",
    "            size=(input_size, hidden_size), high=0.01, low=-0.01)\n",
    "        self.b_h = np.zeros(hidden_size)\n",
    "        self.W_o = np.random.uniform(\n",
    "            size=(hidden_size, output_size), high=0.01, low=-0.01)\n",
    "        self.b_o =np.zeros(output_size)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        h = sigmoid(np.dot(X, self.W_h) + self.b_h)\n",
    "        y = softmax(np.dot(h, self.W_o) + self.b_o)\n",
    "        return y\n",
    "    \n",
    "    def forward_keep_activations(self, X):\n",
    "        z_h = np.dot(X, self.W_h) + self.b_h\n",
    "        h = sigmoid(z_h)\n",
    "        z_o = np.dot(h, self.W_o) + self.b_o\n",
    "        y = softmax(z_o)\n",
    "        return y, h, z_h\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        return nll(one_hot(self.output_size, y), self.forward(X))\n",
    "\n",
    "    def grad_loss(self, X, y_true):\n",
    "        y, h, z_h = self.forward_keep_activations(X)\n",
    "        grad_z_o = y - one_hot(self.output_size, y_true)\n",
    "        grad_W_o = np.outer(h, grad_z_o)\n",
    "        grad_b_o = grad_z_o\n",
    "        grad_h = np.dot(grad_z_o, np.transpose(self.W_o))\n",
    "        grad_z_h = grad_h * dsigmoid(z_h)\n",
    "        grad_W_h = np.outer(x, grad_z_h)\n",
    "        grad_b_h = grad_z_h\n",
    "        grads = {\"W_h\": grad_W_h, \"b_h\": grad_b_h,\n",
    "                 \"W_o\": grad_W_o, \"b_o\": grad_b_o}\n",
    "        return grads\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD update on one sample at a time\n",
    "        grads = self.grad_loss(x, y)\n",
    "        self.W_h = self.W_h - learning_rate * grads[\"W_h\"]\n",
    "        self.b_h = self.b_h - learning_rate * grads[\"b_h\"]\n",
    "        self.W_o = self.W_o - learning_rate * grads[\"W_o\"]\n",
    "        self.b_o = self.b_o - learning_rate * grads[\"b_o\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 10\n",
    "model = NeuralNet(n_features, n_hidden, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W_h': 0.0, 'b_h': 0.0, 'W_o': 0.0, 'b_o': 0.0}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grad_loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies, accuracies_test = [], [], []\n",
    "losses.append(model.loss(X_train, y_train))\n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        model.train(x, y, 0.1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "    print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "          % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Training loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Exercises\n",
    "\n",
    "### Look at worst prediction errors\n",
    "\n",
    "- Use numpy to find test samples for which the model made the worst predictions,\n",
    "- Use the `plot_prediction` to look at the model predictions on those,\n",
    "- Would you have done any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/worst_predictions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters settings\n",
    "\n",
    "- Experiment with different hyper parameters:\n",
    "  - learning rate,\n",
    "  - size of hidden layer,\n",
    "  - initialization scheme: test with 0 initialization vs uniform,\n",
    "  - implement other activation functions,\n",
    "  - implement the support for a second hidden layer.\n",
    "\n",
    "\n",
    "### Mini-batches\n",
    "\n",
    "- The current implementations of `train` and `grad_loss` function currently only accept a single sample at a time:\n",
    "    - implement the support for training with a mini-batch of 32 samples at a time instead of one,\n",
    "    - experiment with different sizes of batches,\n",
    "    - monitor the norm of the average gradients on the full training set at the end of each epoch.\n",
    "\n",
    "\n",
    "### Momentum\n",
    "\n",
    "- Bonus: Implement momentum\n",
    "\n",
    "\n",
    "### Back to Keras\n",
    "\n",
    "- Implement the same network architecture with Keras;\n",
    "\n",
    "- Check that the Keras model can approximately reproduce the behavior of the Numpy model when using similar hyperparameter values (size of the model, type of activations, learning rate value and use of momentum);\n",
    "\n",
    "- Compute the negative log likelihood of a sample 42 in the test set (can use `model.predict_proba`);\n",
    "\n",
    "- Compute the average negative log-likelihood on the full test set.\n",
    "\n",
    "- Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "- Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 50 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_model_test_loss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework assignments\n",
    "\n",
    "- Watch the following video on [how to code a minimal deep learning framework](https://www.youtube.com/watch?v=o64FV-ez6Gw) that feels like a simplified version\n",
    "of Keras but using numpy instead of tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"o64FV-ez6Gw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Optional**: read the following blog post on Reverse-Mode Automatic Differentiation from start to section \"A simple implementation in Python\" included:\n",
    "\n",
    "  https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
